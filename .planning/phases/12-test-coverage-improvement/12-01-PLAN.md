---
phase: 12
plan: 01
title: Unit Test Coverage for Critical Modules
created: 2026-01-28
---

# Plan 12-01: Unit Test Coverage for Critical Modules

**Goal:** Improve unit test coverage for 4 critical modules from below 80% to 85%+ each

**Current Coverage Gaps:**
- SignalClient: 55% → Target: 85%
- ClaudeProcess: 70% → Target: 85%
- Daemon: 71% → Target: 85%
- Orchestrator: 73% → Target: 85%

**Context:** Phase 10 verification identified these modules as below the 80% threshold. While integration tests provide comprehensive coverage, dedicated unit tests improve debugging, regression detection, and code maintainability.

---

## TDD Strategy

Follow strict RED-GREEN-REFACTOR for all new tests:

1. **RED:** Write failing unit test for uncovered code path
2. **GREEN:** Verify test passes against existing implementation
3. **REFACTOR:** Improve test clarity if needed

**Test-First Order:**
1. Analyze coverage reports to identify specific uncovered lines
2. Write unit tests for uncovered error paths and edge cases
3. Verify tests pass (code already implements functionality)
4. Run full coverage report to confirm improvements

---

## Tasks

### Task 1: Improve SignalClient Coverage (55% → 85%)

**Current Gap Analysis:**

Run coverage report to identify uncovered lines:
```bash
pytest tests/test_signal_client.py --cov=src/signal/client --cov-report=term-missing
```

**Expected Uncovered Areas:**
- Error handling in `_sync_session_state()` (placeholder implementation)
- WebSocket reconnection error paths
- Message buffer overflow scenarios
- Connection state transitions with concurrent operations

**New Tests to Write:**

1. **test_sync_session_state_error_handling**
   - Test: Call _sync_session_state when session_manager not set
   - Expected: Logs warning, returns without error
   - Coverage: Error path in placeholder implementation

2. **test_reconnection_with_failed_backoff**
   - Test: Simulate reconnection attempts that all fail
   - Expected: State remains DISCONNECTED, backoff increases to max
   - Coverage: Reconnection failure paths

3. **test_message_buffer_concurrent_drain**
   - Test: Multiple coroutines attempt to drain buffer simultaneously
   - Expected: Only one drain succeeds, others get empty list
   - Coverage: Concurrent access edge case

4. **test_connection_state_race_condition**
   - Test: State transitions while receive_messages is iterating
   - Expected: Clean state transition, no exceptions
   - Coverage: Concurrent state changes during message receipt

5. **test_websocket_connection_refused**
   - Test: WebSocket connection fails with ConnectionRefusedError
   - Expected: Transitions to DISCONNECTED, triggers auto-reconnect
   - Coverage: Connection error handling

**Target Lines:** ~50 additional lines covered

**Files:**
- Edit: `tests/test_signal_client.py` (add 5 tests)

**Success Criteria:**
- SignalClient coverage ≥85%
- All new tests pass
- No regressions

---

### Task 2: Improve ClaudeProcess Coverage (70% → 85%)

**Current Gap Analysis:**

Run coverage report:
```bash
pytest tests/test_claude_process.py --cov=src/claude/process --cov-report=term-missing
```

**Expected Uncovered Areas:**
- Process spawn failures (subprocess creation errors)
- Graceful shutdown with unresponsive process (SIGKILL path)
- Bridge creation edge cases
- Working directory validation errors

**New Tests to Write:**

1. **test_start_subprocess_creation_failure**
   - Test: Mock subprocess creation to raise OSError
   - Expected: Raises ClaudeProcessError with descriptive message
   - Coverage: Subprocess spawn error path

2. **test_stop_sigterm_timeout_triggers_sigkill**
   - Test: Process ignores SIGTERM, requires SIGKILL
   - Expected: SIGTERM sent, 5s wait, SIGKILL sent, process terminated
   - Coverage: Graceful shutdown timeout path

3. **test_bridge_none_before_start**
   - Test: Call get_bridge() before start()
   - Expected: Returns None (bridge not available yet)
   - Coverage: Bridge lifecycle edge case

4. **test_working_directory_not_exists**
   - Test: Start process with non-existent working directory
   - Expected: Raises ClaudeProcessError before subprocess spawn
   - Coverage: Working directory validation

5. **test_process_crashed_after_start**
   - Test: Process crashes immediately after spawn
   - Expected: is_running() returns False, stop() handles gracefully
   - Coverage: Crash detection and cleanup

**Target Lines:** ~40 additional lines covered

**Files:**
- Edit: `tests/test_claude_process.py` (add 5 tests)

**Success Criteria:**
- ClaudeProcess coverage ≥85%
- All new tests pass
- No regressions

---

### Task 3: Improve Daemon Coverage (71% → 85%)

**Current Gap Analysis:**

Run coverage report:
```bash
pytest tests/test_daemon.py --cov=src/daemon/service --cov-report=term-missing
```

**Expected Uncovered Areas:**
- Health check server error handling
- Graceful shutdown with active sessions
- Component initialization failures
- Message processing error paths

**New Tests to Write:**

1. **test_health_server_port_already_in_use**
   - Test: Health server port 8081 already bound
   - Expected: Logs error, daemon continues (health check optional)
   - Coverage: Health server startup error path

2. **test_shutdown_with_active_sessions**
   - Test: Call shutdown() while sessions are active
   - Expected: All sessions terminated cleanly, processes stopped
   - Coverage: Session cleanup during shutdown

3. **test_thread_mapper_initialization_failure**
   - Test: ThreadMapper.initialize() raises exception
   - Expected: Daemon logs error, continues without thread mapping
   - Coverage: Component initialization error handling

4. **test_message_processing_with_invalid_json**
   - Test: Receive malformed Signal message (invalid JSON)
   - Expected: Logs error, continues processing other messages
   - Coverage: Message validation error path

5. **test_concurrent_session_creation**
   - Test: Multiple threads attempt to create sessions simultaneously
   - Expected: All sessions created successfully, no race conditions
   - Coverage: Concurrent operation safety

**Target Lines:** ~45 additional lines covered

**Files:**
- Edit: `tests/test_daemon.py` (add 5 tests)

**Success Criteria:**
- Daemon coverage ≥85%
- All new tests pass
- No regressions

---

### Task 4: Improve Orchestrator Coverage (73% → 85%)

**Current Gap Analysis:**

Run coverage report:
```bash
pytest tests/test_claude_orchestrator.py --cov=src/claude/orchestrator --cov-report=term-missing
```

**Expected Uncovered Areas:**
- Approval workflow timeout handling
- Custom command execution errors
- Response formatting edge cases
- Bridge communication failures

**New Tests to Write:**

1. **test_execute_command_bridge_none**
   - Test: Call execute_command() when bridge is None
   - Expected: Logs error, returns early without attempting command
   - Coverage: Bridge availability check

2. **test_approval_timeout_during_execution**
   - Test: Approval request times out (10 minutes)
   - Expected: Operation rejected, user notified, execution skipped
   - Coverage: Approval timeout path

3. **test_custom_command_not_found**
   - Test: execute_custom_command() for non-existent command
   - Expected: Logs error, sends "command not found" message to user
   - Coverage: Custom command error handling

4. **test_response_formatting_with_null_output**
   - Test: Parser returns None or empty output
   - Expected: No message sent to Signal, no errors
   - Coverage: Empty response handling

5. **test_bridge_read_exception**
   - Test: bridge.read_response() raises exception mid-stream
   - Expected: Error caught, user notified, orchestrator remains stable
   - Coverage: Bridge communication error path

**Target Lines:** ~35 additional lines covered

**Files:**
- Edit: `tests/test_claude_orchestrator.py` (add 5 tests)

**Success Criteria:**
- Orchestrator coverage ≥85%
- All new tests pass
- No regressions

---

### Task 5: Verify Coverage Improvements

**Run Full Coverage Report:**

```bash
pytest --cov=src --cov-report=term-missing --cov-report=html
```

**Validate Improvements:**
- SignalClient: ≥85% (from 55%)
- ClaudeProcess: ≥85% (from 70%)
- Daemon: ≥85% (from 71%)
- Orchestrator: ≥85% (from 73%)
- Overall: ≥90% (from 89%)

**Generate Coverage Badge:**

Update README or documentation with new coverage percentage.

**Commit Coverage Report:**

```bash
git add .coverage htmlcov/ tests/
git commit -m "test(12-01): improve unit test coverage for critical modules

Coverage improvements:
- SignalClient: 55% → 85%
- ClaudeProcess: 70% → 85%
- Daemon: 71% → 85%
- Orchestrator: 73% → 85%
- Overall: 89% → 90%+

Added 20 unit tests covering error paths and edge cases.
All tests pass, no regressions.

Generated with [Claude Code](https://claude.ai/code)
via [Happy](https://happy.engineering)

Co-Authored-By: Claude <noreply@anthropic.com>
Co-Authored-By: Happy <yesreply@happy.engineering>"
```

**Files:**
- Read: Coverage reports (HTML and terminal output)
- Verify: All 4 modules ≥85%
- Verify: Overall coverage ≥90%

**Success Criteria:**
- All target coverage thresholds met
- 20 new tests passing
- Coverage report committed
- No test failures

---

## Dependencies

**Depends On:**
- Phase 10 (Testing & Quality) - existing test infrastructure
- Phase 11 (Wiring Fixes) - all functionality working

**Blocks:**
- None (quality improvement, not functional requirement)

**External Dependencies:**
- pytest-cov (already installed)
- Coverage.py (already configured)

---

## Verification Criteria

After completing this plan:

- [ ] SignalClient coverage ≥85% (from 55%)
- [ ] ClaudeProcess coverage ≥85% (from 70%)
- [ ] Daemon coverage ≥85% (from 71%)
- [ ] Orchestrator coverage ≥85% (from 73%)
- [ ] Overall coverage ≥90% (from 89%)
- [ ] 20 new unit tests added (5 per module)
- [ ] All 624+ existing tests still pass (no regressions)
- [ ] All 20 new tests pass
- [ ] Coverage report committed to repository
- [ ] HTML coverage report generated for detailed analysis

**Expected Outcome:**
- All critical modules meet 80% coverage threshold
- Improved debugging and regression detection
- Better code maintainability
- Enhanced quality metrics for v1.0 deployment

---

## Notes

**Why This Matters:**

While integration tests provide excellent functional coverage, unit tests offer:
- Faster feedback (run in milliseconds vs seconds)
- Precise failure localization (exact line/function that broke)
- Better documentation of expected behavior
- Easier debugging (isolated test cases)

**What We're NOT Doing:**

- Not rewriting existing integration tests (they're valuable)
- Not aiming for 100% coverage (diminishing returns)
- Not testing trivial getters/setters
- Not testing framework code (asyncio, websockets internals)

**85% Target Rationale:**

- 80% is the minimum acceptable threshold (CI enforced)
- 85% provides buffer above threshold
- 90%+ often requires testing trivial code paths
- Integration tests compensate for remaining gaps

**Estimated Time:** 45-60 minutes (4-5 tests per module)

---

_Created: 2026-01-28_
_Phase: 12 (Test Coverage Improvement)_
_Priority: Quality Enhancement (Non-Blocking for v1.0)_
