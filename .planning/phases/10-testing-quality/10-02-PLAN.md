---
phase: 10-testing-quality
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/integration/test_signal_claude_flow.py
  - tests/integration/test_session_workflow.py
  - tests/integration/test_approval_workflow.py
  - tests/integration/conftest.py
autonomous: true

must_haves:
  truths:
    - "Integration tests verify end-to-end Signal â†’ Claude Code â†’ Signal flows"
    - "Session workflows tested from creation through command execution to termination"
    - "Approval workflows tested from detection through user approval to execution"
    - "Integration tests use realistic Signal message payloads and Claude responses"
  artifacts:
    - path: "tests/integration/test_signal_claude_flow.py"
      provides: "End-to-end communication flow tests"
      min_lines: 100
      exports: ["test_message_routing", "test_streaming_response", "test_error_propagation"]
    - path: "tests/integration/test_session_workflow.py"
      provides: "Complete session lifecycle tests"
      min_lines: 80
      exports: ["test_session_creation", "test_session_resume", "test_crash_recovery"]
    - path: "tests/integration/test_approval_workflow.py"
      provides: "Approval gate integration tests"
      min_lines: 60
      exports: ["test_approval_flow", "test_approval_timeout", "test_batch_approval"]
    - path: "tests/integration/conftest.py"
      provides: "Integration test fixtures and helpers"
      min_lines: 50
      contains: "@pytest.fixture"
  key_links:
    - from: "test_signal_claude_flow.py"
      to: "SignalClient + ClaudeOrchestrator"
      via: "mocked Signal API + real orchestrator"
      pattern: "signal_client.*orchestrator"
    - from: "test_session_workflow.py"
      to: "SessionManager + ClaudeProcess"
      via: "real database + mocked subprocess"
      pattern: "session_manager.*claude_process"
    - from: "test_approval_workflow.py"
      to: "ApprovalWorkflow + ApprovalManager"
      via: "real state machine + mocked Signal notification"
      pattern: "approval_workflow.*notify"
---

<objective>
Create comprehensive integration test suite validating Signal â†” Claude Code communication flows and complete user workflows.

Purpose: Ensure end-to-end functionality works correctly when components interact. Integration tests verify the system as users experience it, not just isolated units.

Output: Integration test suite in tests/integration/ covering critical user journeys and component interactions.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md

# Phase 10 requirements this plan addresses:
# - TEST-02: Integration tests verify Signal â†” Claude Code communication flows
# - TEST-06: End-to-end tests validate complete user workflows
# - TEST-08: Test fixtures provide realistic Signal/Claude message payloads

# Key components to integrate:
@src/signal/client.py
@src/claude/orchestrator.py
@src/session/manager.py
@src/approval/workflow.py
@src/daemon/service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration test infrastructure</name>
  <files>tests/integration/conftest.py</files>
  <action>
    Set up integration test fixtures in tests/integration/conftest.py:
    1. Create directory: `mkdir -p tests/integration`
    2. Define fixtures:
       - `@pytest.fixture async def signal_client_mock()` - Mock signal-cli-rest-api WebSocket
       - `@pytest.fixture async def test_db()` - Temporary SQLite databases for sessions/threads
       - `@pytest.fixture async def temp_project_dir()` - Temporary directory for test projects
       - `@pytest.fixture def sample_signal_messages()` - Realistic Signal message payloads
       - `@pytest.fixture def sample_claude_output()` - Sample Claude CLI streaming output
       - `@pytest.fixture async def daemon_components()` - Initialize daemon with test config

    Fixtures should:
    - Use temporary directories for SQLite databases (cleanup after test)
    - Mock external APIs (signal-cli-rest-api WebSocket) but use real internal components
    - Provide realistic test data matching actual Signal/Claude message formats
    - Enable async testing with pytest-asyncio
  </action>
  <verify>
    - `pytest tests/integration/conftest.py --collect-only` shows fixtures
    - Fixtures create/cleanup temp resources without leaking
  </verify>
  <done>Integration test infrastructure ready with reusable fixtures</done>
</task>

<task type="auto">
  <name>Task 2: Implement Signal â†” Claude integration tests</name>
  <files>tests/integration/test_signal_claude_flow.py</files>
  <action>
    Create end-to-end communication flow tests:
    1. `test_message_routing_to_claude()`:
       - Send Signal message â†’ SessionCommands â†’ ClaudeOrchestrator
       - Verify command reaches Claude bridge
       - Mock Claude response streaming
       - Verify response batching and Signal message sending
    2. `test_streaming_response_batching()`:
       - Mock Claude streaming output (tool calls, progress, results)
       - Verify ClaudeResponder batches into <1600 char Signal messages
       - Verify emoji markers for tool calls (ðŸ“– Read, âœï¸ Edit, etc.)
    3. `test_error_propagation()`:
       - Simulate Claude CLI error (process crash, timeout)
       - Verify error notification sent to Signal
       - Verify session state reflects error
    4. `test_approval_gate_integration()`:
       - Send command requiring approval (Edit file)
       - Verify approval request sent to Signal
       - Mock approval response
       - Verify command execution resumes

    Use realistic payloads from fixtures. Test both happy path and error scenarios.
  </action>
  <verify>
    - `pytest tests/integration/test_signal_claude_flow.py -v` passes
    - All 4+ test scenarios covered
    - Tests use real orchestrator/responder logic, mocked external APIs
  </verify>
  <done>Signal-Claude integration tests complete and passing</done>
</task>

<task type="auto">
  <name>Task 3: Implement session workflow integration tests</name>
  <files>tests/integration/test_session_workflow.py</files>
  <action>
    Create complete session lifecycle tests:
    1. `test_session_creation_to_termination()`:
       - User sends `/session start /path/to/project`
       - SessionManager creates session in DB
       - ClaudeProcess spawns subprocess
       - User sends commands â†’ responses stream back
       - User sends `/session stop <id>`
       - Process terminates, session state updates
    2. `test_session_resume_with_history()`:
       - Create session with conversation history in context
       - Stop session
       - Resume session with `/session resume <id>`
       - Verify history restoration (activity_log from context)
       - Send new command, verify continuity
    3. `test_crash_recovery_on_startup()`:
       - Create session, mark as ACTIVE (simulate crash)
       - Restart daemon (recovery.recover() runs)
       - Verify session transitioned to CRASHED
       - Verify user notification sent
    4. `test_concurrent_sessions()`:
       - Start 3 sessions for different projects
       - Send commands to each in parallel
       - Verify no cross-contamination (thread_id isolation)

    Use real SessionManager + SQLite, mock ClaudeProcess subprocess spawn.
  </action>
  <verify>
    - `pytest tests/integration/test_session_workflow.py -v` passes
    - All 4+ test scenarios covered
    - Tests verify database state transitions
  </verify>
  <done>Session workflow integration tests complete and passing</done>
</task>

<task type="auto">
  <name>Task 4: Implement approval workflow integration tests</name>
  <files>tests/integration/test_approval_workflow.py</files>
  <action>
    Create approval gate integration tests:
    1. `test_destructive_operation_approval_flow()`:
       - Orchestrator detects Edit tool call
       - ApprovalDetector classifies as DESTRUCTIVE
       - ApprovalWorkflow creates request
       - ApprovalManager stores in pending state
       - Notification sent to Signal (mocked)
       - User sends `/approve <id>`
       - Tool call executes
       - User receives completion notification
    2. `test_approval_timeout_handling()`:
       - Create approval request
       - Fast-forward time by 10+ minutes (mock time.time)
       - Verify request transitions to TIMED_OUT
       - Verify user notification sent
       - Verify operation skipped
    3. `test_batch_approval()`:
       - Create 3 pending approval requests
       - User sends `/approve all`
       - Verify all 3 approve atomically
       - Verify execution order preserved
    4. `test_emergency_mode_auto_approval()`:
       - Activate emergency mode
       - Trigger SAFE operation (Read)
       - Verify auto-approval (no user prompt)
       - Trigger DESTRUCTIVE operation (Edit)
       - Verify still requires approval (safety boundary)

    Use real ApprovalWorkflow + ApprovalManager, mock Signal notifications.
  </action>
  <verify>
    - `pytest tests/integration/test_approval_workflow.py -v` passes
    - All 4+ test scenarios covered
    - Tests verify state machine transitions
  </verify>
  <done>Approval workflow integration tests complete and passing</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pytest tests/integration/ -v` runs all integration tests
- [ ] All integration tests pass
- [ ] Tests use realistic Signal/Claude payloads from fixtures
- [ ] Integration test coverage includes happy path and error scenarios
</verification>

<success_criteria>
- All tasks completed
- Integration tests verify Signal â†” Claude Code communication
- Session workflows tested end-to-end
- Approval workflows tested with timeouts and batch operations
- Realistic test fixtures established for future tests
</success_criteria>

<output>
After completion, create `.planning/phases/10-testing-quality/10-02-SUMMARY.md`
</output>
